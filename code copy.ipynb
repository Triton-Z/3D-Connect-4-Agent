{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "%pip install tensorflow numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import math\n",
    "from collections import deque "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments for training the model\n",
    "args = {\n",
    "    'numIterations': 10000, # Total number of training iterations\n",
    "    'numSimulations': 500, # MCTS simulations per move\n",
    "    'epochs': 10, # Epochs per training step\n",
    "    'batchSize': 64, # Batch size for training\n",
    "    'maxBufferSize': 50000, # Maximum replay buffer (memory) size\n",
    "    'cPuct': 1.25, # Exploration constant for MCTS\n",
    "    'temperatureMoves': 12, # Temperature for move selection\n",
    "    'dirichletAlpha': 0.3, # Dirichlet noise alpha for exploration\n",
    "    'dirichletEpsilon': 0.25, # Dirichlet noise epsilon for exploration\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main game engine\n",
    "class Game:\n",
    "    def __init__(self, width=7, length=7, height=6, winLength=4):\n",
    "        self.width = width\n",
    "        self.length = length\n",
    "        self.height = height\n",
    "        self.winLength = winLength\n",
    "        self.directions = directions = [\n",
    "            (1, 0, 0),\n",
    "            (0, 1, 0),\n",
    "            (0, 0, 1),\n",
    "            (1, 1, 0),\n",
    "            (1, -1, 0),\n",
    "            (1, 0, 1),\n",
    "            (1, 0, -1),\n",
    "            (0, 1, 1),\n",
    "            (0, -1, 1),\n",
    "            (1, 1, 1),\n",
    "            (1, 1, -1),\n",
    "            (1, -1, 1),\n",
    "            (-1, 1, 1),\n",
    "        ]  # Possible directions to check for winning conditions\n",
    "\n",
    "    # Initialize the game state\n",
    "    def getInitialState(self):\n",
    "        return np.zeros((self.height, self.width, self.length), dtype=np.int8)\n",
    "\n",
    "    # Get possible moves for a state\n",
    "    def getValidMoves(self, state):\n",
    "        height, width, length = state.shape\n",
    "        valid = []\n",
    "        for x in range(width):\n",
    "            for z in range(length):\n",
    "                # Check the top of the column at (x, z)\n",
    "                if state[height - 1][x][z] == 0:\n",
    "                    valid.append((x, z))\n",
    "        return valid\n",
    "\n",
    "    # Get the next state based off of a move\n",
    "    def getNextState(self, state, player, action):\n",
    "        nState = state.copy()\n",
    "        x, z = action\n",
    "        for layer in range(self.height):\n",
    "            if nState[layer][x][z] == 0:\n",
    "                nState[layer][x][z] = player\n",
    "                return nState\n",
    "        return nState\n",
    "\n",
    "    # Check if game has ended\n",
    "    def gameEnded(self, state):\n",
    "        height, width, length = state.shape\n",
    "\n",
    "        # Iterate through every cell on the board\n",
    "        for y in range(height):\n",
    "            for x in range(width):\n",
    "                for z in range(length):\n",
    "                    player = state[y][x][z]\n",
    "\n",
    "                    if player == 0:\n",
    "                        continue\n",
    "\n",
    "                    for dy, dx, dz in self.directions:\n",
    "                        count = 1\n",
    "                        for i in range(1, self.winLength):\n",
    "                            ny, nx, nz = y + i * dy, x + i * dx, z + i * dz\n",
    "                            if (\n",
    "                                0 <= ny < height\n",
    "                                and 0 <= nx < width\n",
    "                                and 0 <= nz < length\n",
    "                            ):\n",
    "                                if state[ny][nx][nz] == player:\n",
    "                                    count += 1\n",
    "                                else:\n",
    "                                    break\n",
    "                            else:\n",
    "                                break\n",
    "\n",
    "                        if count == self.winLength:\n",
    "                            return player\n",
    "\n",
    "        if len(self.getValidMoves(state)) == 0:\n",
    "            return 0\n",
    "\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main neural network model for the game\n",
    "class NeuralNetwork:\n",
    "    def __init__(self):\n",
    "        inputs = layers.Input(shape=(6, 7, 7, 3))\n",
    "\n",
    "        # Bigger Kernel helps to find more useful patterns\n",
    "        x = layers.Conv3D(\n",
    "            filters=64, kernel_size=(3, 3, 3), padding=\"same\", name=\"inputLayer\"\n",
    "        )(inputs)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "        # Start setting up the residual blocks\n",
    "        for i in range(4):\n",
    "            x = self.createResidialBlock(x)\n",
    "\n",
    "\n",
    "        # Sets up the outputs for policy and value heads. We need a policy head to predict the next move, and a value head to predict the game outcome.\n",
    "        policyHead = layers.Conv3D(filters=2, kernel_size=(1, 1, 1), padding=\"same\", name=\"policy\")(x)\n",
    "        policyHead = layers.BatchNormalization()(policyHead)\n",
    "        policyHead = layers.Activation(\"relu\")(policyHead)\n",
    "        policyHead = layers.Flatten()(policyHead)\n",
    "        policyOut = layers.Dense(units=49, activation=\"softmax\", name=\"policyOut\")(policyHead) # 49 units for the 49 possible moves\n",
    "\n",
    "        valueHead = layers.Conv3D(filters=1, kernel_size=(1, 1, 1), padding=\"same\", name=\"value\")(x)\n",
    "        valueHead = layers.BatchNormalization()(valueHead)\n",
    "        valueHead = layers.Activation(\"relu\")(valueHead)\n",
    "        valueHead = layers.Flatten()(valueHead)\n",
    "        valueHead = layers.Dense(units=64, activation=\"relu\")(valueHead)\n",
    "        valueOut = layers.Dense(units=1, activation=\"tanh\", name=\"valueOut\")(valueHead) # Single value output\n",
    "\n",
    "        # Finishing the model\n",
    "        self.model = keras.Model(inputs=inputs, outputs=[policyOut, valueOut])\n",
    "        self.model.compile(\n",
    "            optimizer=\"adam\",\n",
    "            loss={\n",
    "                \"policyOut\": \"categorical_crossentropy\",\n",
    "                \"valueOut\": \"mean_squared_error\",\n",
    "            },\n",
    "        )\n",
    "\n",
    "    # Creates a single residual block\n",
    "    def createResidialBlock(self, inputLayer, filters=64):\n",
    "        residual = inputLayer\n",
    "        x = layers.Conv3D(filters=filters, kernel_size=(3, 3, 3), padding=\"same\")(\n",
    "            inputLayer\n",
    "        )\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "        x = layers.Conv3D(filters=filters, kernel_size=(3, 3, 3), padding=\"same\")(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "        # Adds skips (prevents vanishing gradients)\n",
    "        x = layers.Add()([x, residual])\n",
    "        x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to encode states for the neural network\n",
    "def encodeState(state, playerPerspective):\n",
    "    # Creates matrix of where player pieces are\n",
    "    p = np.zeros_like(state, dtype=np.float32)\n",
    "    p[state == playerPerspective] = 1\n",
    "\n",
    "    # Creates matrix of where opponentt pieces are\n",
    "    o = np.zeros_like(state, dtype=np.float32)\n",
    "    o[state == -playerPerspective] = 1\n",
    "\n",
    "    # Creates matrix of whose turn it is\n",
    "    turn = np.full_like(state, playerPerspective, dtype=np.float32)\n",
    "\n",
    "    return np.stack([p, o, turn])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A single node in the MCTS tree\n",
    "class Node:\n",
    "    def __init__(self, state, player, parent=None, move=None, priorP=0):\n",
    "        # Initialize the node. Sets up values required for MCTS calculations.\n",
    "        self.state = state\n",
    "        self.player = player\n",
    "        self.parent = parent\n",
    "        self.move = move\n",
    "        self.visits = 0\n",
    "        self.value = 0.0\n",
    "        self.priorP = priorP\n",
    "        self.children = []\n",
    "        self.expanded = False\n",
    "\n",
    "    # Gets node's mean value\n",
    "    def getMeanValue(self):\n",
    "        return 0 if self.visits == 0 else self.value / self.visits\n",
    "\n",
    "    # Get a node's score\n",
    "    def getScore(self, cPuct):\n",
    "        # Models after the selection equation: Q = c_puct * P(s, a) * sqrt( sum_b N(s, b) / (1 + N(s, a)) )\n",
    "        qValue = self.getMeanValue()\n",
    "        parentVisits = self.parent.visits if self.parent else 1\n",
    "        eT = cPuct * self.priorP * math.sqrt(parentVisits) / (1 + self.visits)\n",
    "\n",
    "        # qValue is being flipped because it's based off of exploitation (and as the next move would be the opponent, not ours). eT is untouched, as it's the term for exploration.\n",
    "        return -qValue + eT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class for the Monte Carlo Tree Search algorithm\n",
    "class MCTS:\n",
    "    def __init__(self, game, model, args):\n",
    "        # Initialize the MCTS with the game, model, and arguments.\n",
    "        self.game = game\n",
    "        self.model = model\n",
    "        self.args = args\n",
    "\n",
    "    # Searches the MCTS tree for the best move\n",
    "    def search(self, state, player):\n",
    "        if self.game.gameEnded(state) is not None: # Make sure game isnt already over\n",
    "            return Node(state, player)\n",
    "\n",
    "        root = Node(state, player, priorP=1.0)\n",
    "\n",
    "        # Applies Dirichlet noise to the root node's policy (helps with exploration)\n",
    "        encodedState = encodeState(root.state, root.player)\n",
    "        encodedState = np.expand_dims(encodedState, axis=0)\n",
    "        encodedState = np.transpose(encodedState, (0, 2, 3, 4, 1))\n",
    "\n",
    "        policy, _ = self.model(encodedState, training=False)\n",
    "        policy = policy.numpy()[0]\n",
    "\n",
    "        noise = np.random.dirichlet([self.args[\"dirichletAlpha\"]] * len(policy))\n",
    "        policy = (1 - self.args[\"dirichletEpsilon\"]) * policy + self.args[\n",
    "            \"dirichletEpsilon\"\n",
    "        ] * noise\n",
    "\n",
    "        # Initialize children of the root node based on valid moves\n",
    "        validMoves = self.game.getValidMoves(root.state)\n",
    "        for move in validMoves:\n",
    "            moveIndex = move[0] * self.game.length + move[1]\n",
    "            nextState = self.game.getNextState(root.state, root.player, move)\n",
    "            child = Node(\n",
    "                state=nextState,\n",
    "                player=-root.player,\n",
    "                parent=root,\n",
    "                move=move,\n",
    "                priorP=policy[moveIndex],\n",
    "            )\n",
    "            root.children.append(child)\n",
    "        root.expanded = True  # Mark the root as expanded\n",
    "\n",
    "        # Perform MCTS simulations\n",
    "        for simulation in range(self.args[\"numSimulations\"]):\n",
    "            node = root\n",
    "\n",
    "            # 1. Selection Phase: Traverse the tree to find a leaf node\n",
    "            while node.expanded:\n",
    "                childScores = [\n",
    "                    child.getScore(self.args[\"cPuct\"]) for child in node.children\n",
    "                ]\n",
    "                bestChildIndex = np.argmax(childScores)\n",
    "                node = node.children[bestChildIndex]\n",
    "\n",
    "            gameResult = self.game.gameEnded(node.state)\n",
    "\n",
    "            if gameResult is not None: # Make sure game doesn't end at this node\n",
    "                value = (gameResult * node.player)  # Calculate the value based on the outcome of the game the player\n",
    "            else:\n",
    "                # Encodes the state for the neural network\n",
    "                encodedState = encodeState(node.state, node.player)\n",
    "                encodedState = np.expand_dims(encodedState, axis=0)\n",
    "                encodedState = np.transpose(encodedState, (0, 2, 3, 4, 1))\n",
    "\n",
    "                # Feeds NN the encoded state to get policy and value\n",
    "                policy, value = self.model(encodedState, training=False)\n",
    "                value = value.numpy()[0][0]\n",
    "                policy = policy.numpy()[0]\n",
    "\n",
    "                # 2 + 3. Expansion/Simulation Phase: Adds children to the node\n",
    "                validMoves = self.game.getValidMoves(node.state)\n",
    "                for move in validMoves:\n",
    "                    moveIndex = (move[0] * self.game.length + move[1]) # Maps coordinates to a specific column\n",
    "                    nextState = self.game.getNextState(node.state, node.player, move)\n",
    "                    child = Node(\n",
    "                        state=nextState,\n",
    "                        player=-node.player,\n",
    "                        parent=node,\n",
    "                        move=move,\n",
    "                        priorP=policy[moveIndex],\n",
    "                    )\n",
    "                    node.children.append(child)\n",
    "                node.expanded = True\n",
    "\n",
    "            # 4. Backpropagation Phase: Update the values of the nodes in the path\n",
    "            while node:\n",
    "                node.visits += 1\n",
    "                node.value += value\n",
    "                value = -value  # Player -> Opponent (as turns flip each iteration)\n",
    "                node = node.parent\n",
    "\n",
    "        # Returns the root (contains the entire tree)\n",
    "        return root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class to train the model\n",
    "class Coach:\n",
    "    def __init__(self, game, network, args):\n",
    "        # Initialize the coach with the game, neural network, and arguments.\n",
    "        self.game = game\n",
    "        self.network = network\n",
    "        self.args = args\n",
    "        self.mcts = MCTS(self.game, self.network, self.args)\n",
    "        self.memory = deque(maxlen=self.args['maxBufferSize'])\n",
    "\n",
    "    # Main function to run the training process\n",
    "    def run(self):\n",
    "        # Loops through the total number of iiterations\n",
    "        for i in range(1, self.args['numIterations'] + 1):\n",
    "            print(f\"Starting Iteration {i}\")\n",
    "\n",
    "            # Get examples from a training episode\n",
    "            newExamples = self.executeEpisode()\n",
    "\n",
    "            # Add it to the memory\n",
    "            self.memory.extend(newExamples)\n",
    "\n",
    "            # Trains the model\n",
    "            self.train()\n",
    "\n",
    "            # Save the weights\n",
    "            self.network.save_weights(f\"modelIteration_{i}.weights.h5\")\n",
    "\n",
    "    # A single episode of the game\n",
    "    def executeEpisode(self):\n",
    "        episodeTrainingData = []\n",
    "        currentPlayer = 1\n",
    "        state = self.game.getInitialState()\n",
    "\n",
    "        turn = 0\n",
    "        while True: # Game loop\n",
    "            turn += 1\n",
    "            print(f\"Turn {turn}...\")\n",
    "\n",
    "            root = self.mcts.search(state, currentPlayer) # Runs MCTS search to find the best move\n",
    "\n",
    "            # Selects moves using visits in the tree (more visits = likely a good move)\n",
    "            policy = np.zeros(self.game.width * self.game.length)\n",
    "            for child in root.children:\n",
    "                moveIndex = child.move[0] * self.game.length + child.move[1]\n",
    "                policy[moveIndex] = child.visits\n",
    "\n",
    "            policySum = np.sum(policy) # Total visits\n",
    "            if policySum > 0:\n",
    "                policy /= policySum # Turns to probabilities (divides all the visits out of the total)\n",
    "            else:\n",
    "                print(\"Warning: MCTS search resulted in zero visits. Using uniform policy.\")\n",
    "\n",
    "                # Treat everything equally likely if something went wrong\n",
    "                validMoves = self.game.getValidMoves(state)\n",
    "                policy = np.zeros_like(policy)\n",
    "                if len(validMoves) > 0:\n",
    "                  prob = 1 / len(validMoves)\n",
    "                  for x, z in validMoves:\n",
    "                      policy[x * self.game.length + z] = prob \n",
    "\n",
    "            # Add the move data to trainiinig data\n",
    "            episodeTrainingData.append((state, currentPlayer, policy)) #appends it to train later\n",
    "\n",
    "            # Selects based on temperature (helps with exploration)\n",
    "            if turn < self.args[\"temperatureMoves\"]:\n",
    "                # High temperature: sample randomly according to the policy\n",
    "                actionIndex = np.random.choice(len(policy), p=policy)\n",
    "            else:\n",
    "                # Low temperature: pick the best move (most visited)\n",
    "                actionIndex = np.argmax(policy)\n",
    "            \n",
    "            # Encodes move\n",
    "            move = (actionIndex // self.game.length, actionIndex % self.game.length)\n",
    "            print(move)\n",
    "\n",
    "            state = self.game.getNextState(state, currentPlayer, move) # Moves the state\n",
    "            currentPlayer = -currentPlayer # As it's now the next player's turn\n",
    "\n",
    "            gameResult = self.game.gameEnded(state)\n",
    "\n",
    "            # If the game has ended\n",
    "            if gameResult is not None:\n",
    "                finalExamples = []\n",
    "                for histState, histPlayer, histPolicy in episodeTrainingData:\n",
    "                    value = gameResult * histPlayer\n",
    "\n",
    "                    # Additionally adds the final output of the game to fit while training\n",
    "                    finalExamples.append((\n",
    "                        encodeState(histState, histPlayer),\n",
    "                        histPolicy,\n",
    "                        value\n",
    "                    ))\n",
    "                return finalExamples\n",
    "\n",
    "    # Trains the neural network using the collected memory\n",
    "    def train(self):\n",
    "        batchSize = self.args['batchSize']\n",
    "        if len(self.memory) < batchSize: # Not enough memory to train\n",
    "            return\n",
    "\n",
    "        # Sample random pieces of memory to avoid sequential patterns\n",
    "        sampleIds = np.random.randint(len(self.memory), size=batchSize)\n",
    "        batch = [self.memory[i] for i in sampleIds]\n",
    "\n",
    "        # Helps to prepare the data for training\n",
    "        states, policies, values = zip(*batch)\n",
    "        states = np.array(states)\n",
    "        policies = np.array(policies)\n",
    "        values = np.array(values)\n",
    "        states = np.transpose(states, (0, 2, 3, 4, 1))\n",
    "\n",
    "        # Fit the model with the prepared data\n",
    "        self.network.fit(\n",
    "            states,\n",
    "            {'policyOut': policies, 'valueOut': values},\n",
    "            batch_size=batchSize,\n",
    "            epochs=self.args['epochs'],\n",
    "            verbose=0\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the program\n",
    "\n",
    "game = Game()\n",
    "network = NeuralNetwork()\n",
    "\n",
    "#network.model.load_weights(\"\")\n",
    "\n",
    "coach = Coach(game, network.model, args)\n",
    "coach.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
